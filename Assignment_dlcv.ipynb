{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1VWdH5mGDCwoh8L-a3pWtYQ7S1PhpHdJH",
      "authorship_tag": "ABX9TyPJED5ZHTIxbLy/zKqthn1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atanu22-iitk/DLCV/blob/main/Assignment_dlcv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFhFgGNI5MKS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "2ed22509-128b-432e-89a1-c3eb85f6244a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Showing Data:\n",
            "File contain 0  is: b'batch_label'\n",
            "File contain 1  is: b'labels'\n",
            "File contain 2  is: b'data'\n",
            "File contain 3  is: b'filenames'\n",
            "(10000, 32, 32, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def read_data(file):\n",
        "  import pickle\n",
        "  data=open(file, 'rb')\n",
        "  dict = pickle.load(data, encoding='bytes')\n",
        "  return dict\n",
        "\n",
        "def image_array(image_data,x):\n",
        "    global R_ch,G_ch,B_ch\n",
        "    R_ch=np.array([image_data[b'data'][x,:32]])\n",
        "    G_ch=np.array([image_data[b'data'][x,1024:1056]])\n",
        "    B_ch=np.array([image_data[b'data'][x,2048:2080]])\n",
        "\n",
        "    for i in range(32,1024,32):\n",
        "        data_R=np.array(image_data[b'data'][x,i:i+32])\n",
        "        R_ch=np.append(R_ch, [data_R], axis=0)\n",
        "        i+=32\n",
        "\n",
        "    for i in range(1056,2048,32):\n",
        "        data_G=np.array(image_data[b'data'][x,i:i+32])\n",
        "        G_ch=np.append(G_ch, [data_G], axis=0)\n",
        "        i+=32\n",
        "\n",
        "    for i in range(2080,3072,32):\n",
        "        data_B=np.array(image_data[b'data'][x,i:i+32])\n",
        "        B_ch=np.append(B_ch, [data_B], axis=0)\n",
        "        i+=32\n",
        "\n",
        "    print('Red:',R_ch)\n",
        "    print('Green:',G_ch)\n",
        "    print('Blue:',B_ch)\n",
        "    print(type(R_ch))\n",
        "    \n",
        "    print(R_ch.shape)\n",
        "    print(G_ch.shape)\n",
        "    print(B_ch.shape)\n",
        "    \n",
        "    image=np.dstack((R_ch,G_ch,B_ch))\n",
        "\n",
        "    # print(image)\n",
        "    # print(image[1,1][2])\n",
        "    # # print(image.shape)\n",
        "    # print('Original Image:')\n",
        "    # plt.imshow(image)\n",
        "    # plt.show()\n",
        "    # plt.imshow(R_ch)\n",
        "    # plt.show()\n",
        "    # plt.imshow(G_ch)\n",
        "    # plt.show()\n",
        "    # plt.imshow(B_ch)\n",
        "    # plt.show()\n",
        "    return image\n",
        "\n",
        "def img_poster(image):\n",
        "  #range=r,divider=d\n",
        "  sel_min=5\n",
        "  sel_max=250\n",
        "  r = sel_max - sel_min\n",
        "  d = 255/r\n",
        "  for i in range(0,32,1):\n",
        "    for j in range(0,32,1):\n",
        "      image[i,j][0]=(image[i,j][0]/d) + sel_min\n",
        "      image[i,j][1]=(image[i,j][1]/d) + sel_min\n",
        "      image[i,j][2]=(image[i,j][2]/d) + sel_min\n",
        "  return image\n",
        "\n",
        "def img_enhance(image):\n",
        "  for i in range(0,32,1):\n",
        "    for j in range(0,32,1):\n",
        "      image[i,j][0]=((image[i,j][0]-R_min_max[0])/(R_min_max[1]-R_min_max[0]))*255\n",
        "      image[i,j][1]=((image[i,j][1]-G_min_max[0])/(G_min_max[1]-G_min_max[0]))*255\n",
        "      image[i,j][2]=((image[i,j][2]-B_min_max[0])/(B_min_max[1]-B_min_max[0]))*255\n",
        "\n",
        "  return image\n",
        "\n",
        "def min_max(arr):\n",
        "  min=np.min(arr)\n",
        "  max=np.max(arr)\n",
        "  return min, max\n",
        "\n",
        "def rotate(image):\n",
        "  # Generate a random angle between 0 and 360\n",
        "  angle = np.random.uniform(-180, 180)\n",
        "  k = int(angle / 90)\n",
        "  # Rotate the array\n",
        "  rotated_image = np.rot90(image, k )\n",
        "  # print('Rotating image by '+ str(k) +'times :')\n",
        "  return rotated_image\n",
        "\n",
        "def contrast_flipping(image):\n",
        "  alpha=np.round((np.random.uniform(0.5,2.0)), 2)\n",
        "  print('Changing contrast of the image by '+ str(alpha) +' factors :')\n",
        "  for i in range(0,32,1):\n",
        "    for j in range(0,32,1):\n",
        "      image[i,j][0]=alpha*(image[i,j][0] - 128) + 128\n",
        "      image[i,j][1]=alpha*(image[i,j][1] - 128) + 128\n",
        "      image[i,j][2]=alpha*(image[i,j][2] - 128) + 128\n",
        "  \n",
        "  flipped_image=np.flip(image, axis=0)\n",
        "  return image,flipped_image\n",
        "\n",
        "\n",
        "def augmentation(image_data):\n",
        "  Image_aug_1=[]\n",
        "  Image_1=[]\n",
        "  for i in range(0,10000,1):\n",
        "    image = image_array(image_data,i)\n",
        "    Image_1.append(image)\n",
        "    sel_function = np.random.randint(1,4)\n",
        "  #print(sel_function)\n",
        "    if sel_function==1:\n",
        "      augmented_image=img_enhance(image)\n",
        "    elif sel_function==2:\n",
        "      augmented_image=img_poster(image)\n",
        "    elif sel_function==3:\n",
        "      augmented_image=rotate(image)\n",
        "    else:\n",
        "      augmented_image=contrast_flipping(image)\n",
        "    Image_aug_1.append(augmented_image)\n",
        "    Batch1_aug=np.array(Image_aug_1)\n",
        "    Batch1_original=np.array(Image_1)\n",
        "  # print(Batch1_aug.shape)\n",
        "  print(Batch1_original.shape)\n",
        "\n",
        "  return  Batch1_original, Batch1_aug\n",
        "\n",
        "def main():\n",
        "    file_1='/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-py/data_batch_1'\n",
        "    image_data_1=read_data(file_1)\n",
        "    file_2='/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-py/data_batch_2'\n",
        "    image_data_2=read_data(file_2)\n",
        "    file_3='/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-py/data_batch_3'\n",
        "    image_data_3=read_data(file_3)\n",
        "    file_4='/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-py/data_batch_4'\n",
        "    image_data_4=read_data(file_4)\n",
        "    file_5='/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-py/data_batch_5'\n",
        "    image_data_5=read_data(file_5)\n",
        "    file_6='/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-py/test_batch'\n",
        "    image_test_6=read_data(file_6)\n",
        "    print('Showing Data:')\n",
        "    cnt=0\n",
        "    for item in image_data_1:\n",
        "      print('File contain',cnt,' is:', item)\n",
        "      cnt +=1\n",
        "\n",
        "    new_image = np.array(image_array(image_data_1,2))\n",
        "    plt.imshow(new_image)\n",
        "    plt.show\n",
        "\n",
        "    print(image_data_1[b'labels'])\n",
        "    labels_1 = np.array(image_data_1[b'labels'])\n",
        "    np.save('labels_1.npy', labels_1)\n",
        "    labels_2 = np.array(image_data_2[b'labels'])\n",
        "    np.save('labels_2.npy', labels_2)\n",
        "    labels_3 = np.array(image_data_3[b'labels'])\n",
        "    np.save('labels_3.npy', labels_3)\n",
        "    labels_4 = np.array(image_data_4[b'labels'])\n",
        "    np.save('labels_4.npy', labels_4)\n",
        "    labels_5 = np.array(image_data_5[b'labels'])\n",
        "    np.save('labels_5.npy', labels_5)\n",
        "\n",
        "    # print(labels_1.shape)\n",
        "    # print(labels_2.shape)\n",
        "    # print(labels_3.shape)\n",
        "    # print(labels_4.shape)\n",
        "    # print(labels_5.shape)\n",
        "\n",
        "    global R_min_max, G_min_max, B_min_max\n",
        "    R_min_max=min_max(R_ch)\n",
        "    G_min_max=min_max(G_ch)\n",
        "    B_min_max=min_max(B_ch)\n",
        "    # print('Rmin:',R_min_max[0])\n",
        "    # print('Rmax:',R_min_max[1])\n",
        "    # print('Gmin:',G_min_max[0])\n",
        "    # print('Gmax:',G_min_max[1])\n",
        "    # print('Bmin:',B_min_max[0])\n",
        "    # print('Bmax:',B_min_max[1])\n",
        "\n",
        "    Enhanced_image=img_enhance(new_image)\n",
        "    print('Enhanced Image:')\n",
        "    plt.imshow(Enhanced_image)\n",
        "    plt.show()\n",
        "    print(Enhanced_image[1,1][2])\n",
        "\n",
        "    Posterize_image=img_poster(new_image)\n",
        "    print('Posterized Image:')\n",
        "    plt.imshow(Posterize_image)\n",
        "    plt.show() \n",
        "    print(Posterize_image[1,1][2])\n",
        "\n",
        "  # Navneet sir enter your code here\n",
        "    print('Rotated Image:')\n",
        "    plt.imshow(rotate(new_image))\n",
        "    plt.show()\n",
        "\n",
        "    Edited_image = contrast_flipping(new_image) # this will not be reqd after your part\n",
        "    print('Edited Image:')\n",
        "    plt.imshow(Edited_image[0])\n",
        "    plt.show()\n",
        "  # Navneet sir enter your code here\n",
        "    print('Flipped Image:')\n",
        "    plt.imshow(Edited_image[1])\n",
        "    plt.show()\n",
        "\n",
        "    ImageSet_1 = augmentation(image_data_1)\n",
        "    np.save('ImageSet_original_1.npy',ImageSet_1[0])\n",
        "    np.save('ImageSet_augmented_1.npy',ImageSet_1[1])\n",
        "\n",
        "    ImageSet_2 = augmentation(image_data_2)\n",
        "    np.save('ImageSet_original_2.npy',ImageSet_2[0])\n",
        "    np.save('ImageSet_augmented_2.npy',ImageSet_2[1])\n",
        "\n",
        "    ImageSet_3 = augmentation(image_data_3)\n",
        "    np.save('ImageSet_original_3.npy',ImageSet_3[0])\n",
        "    np.save('ImageSet_augmented_3.npy',ImageSet_3[1])\n",
        "\n",
        "    ImageSet_4 = augmentation(image_data_4)\n",
        "    np.save('ImageSet_original_4.npy',ImageSet_4[0])\n",
        "    np.save('ImageSet_augmented_4.npy',ImageSet_4[1])\n",
        "\n",
        "    ImageSet_5 = augmentation(image_data_5)\n",
        "    np.save('ImageSet_original_5.npy',ImageSet_5[0])\n",
        "    np.save('ImageSet_augmented_5.npy',ImageSet_5[1])\n",
        "\n",
        "    ImageSet_6 = augmentation(image_test_6)\n",
        "    np.save('ImageSet_test_5.npy',ImageSet_6[0])\n",
        "    \n",
        "\n",
        "# driver code\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "  # function call\n",
        "  main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction and MLP Implementation"
      ],
      "metadata": {
        "id": "0V9qIyozYfNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torchvision.models import resnet18\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import to_categorical\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def read_data(file):\n",
        "  import pickle\n",
        "  data=open(file, 'rb')\n",
        "  dict = pickle.load(data, encoding='bytes')\n",
        "  return dict\n",
        "\n",
        "\n",
        "\n",
        "def get_name_to_module(model):\n",
        "    name_to_module = {}\n",
        "    for m in model.named_modules():\n",
        "        name_to_module[m[0]] = m[1]\n",
        "    return name_to_module\n",
        "\n",
        "\n",
        "def get_activation(all_outputs, name):\n",
        "    def hook(model, input, output):\n",
        "        all_outputs[name] = output.detach()\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "def add_hooks(model, outputs, output_layer_names):\n",
        "    \"\"\"\n",
        "    :param model:\n",
        "    :param outputs: Outputs from layers specified in `output_layer_names` will be stored in `output` variable\n",
        "    :param output_layer_names:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    name_to_module = get_name_to_module(model)\n",
        "    for output_layer_name in output_layer_names:\n",
        "        name_to_module[output_layer_name].register_forward_hook(get_activation(outputs, output_layer_name))\n",
        "\n",
        "\n",
        "class ModelWrapper(nn.Module):\n",
        "    def __init__(self, model, output_layer_names, return_single=True):\n",
        "        super(ModelWrapper, self).__init__()\n",
        "        \n",
        "        self.model = model\n",
        "        self.output_layer_names = output_layer_names\n",
        "        self.outputs = {}\n",
        "        self.return_single = return_single\n",
        "        add_hooks(self.model, self.outputs, self.output_layer_names)\n",
        "\n",
        "    def forward(self, images):\n",
        "        self.model(images)\n",
        "        output_vals = [self.outputs[output_layer_name] for output_layer_name in self.output_layer_names]\n",
        "        if self.return_single:\n",
        "            return output_vals[0]\n",
        "        else:\n",
        "            return output_vals\n",
        "\n",
        "class BBResNet18(object):\n",
        "    def __init__(self):\n",
        "        self.model = resnet18(pretrained=True)\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.eval()\n",
        "\n",
        "        self.model = ModelWrapper(self.model, ['avgpool'], True)\n",
        "\n",
        "        self.model.eval()\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def feature_extraction(self, x:np.ndarray):\n",
        "        '''\n",
        "            param:\n",
        "                x: numpy ndarray of shape: [None, 3, 224, 224] and dtype: np.float32\n",
        "            \n",
        "            return:\n",
        "                numpy ndarray (feature vector) of shape: [None, 512] and dtype: np.float32\n",
        "        '''\n",
        "        \n",
        "        x = torch.from_numpy(x).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            out = self.model(x).cpu().detach()\n",
        "            out = out.view(out.size(0), -1)\n",
        "            out = out.numpy()\n",
        "\n",
        "        return out\n",
        "\n",
        "    def feature_resize(file,x):\n",
        "        X_train = file\n",
        "        features=[]\n",
        "        for i in range(len(X_train)//1000):\n",
        "          batches=[]\n",
        "          for j in range(1,1001,1):\n",
        "            img_resized = np.resize(X_train[j-1],(3,224,224))\n",
        "            batches.append(img_resized)\n",
        "         \n",
        "          batches=np.array(batches)\n",
        "          batches = batches.astype('float32')\n",
        "\n",
        "          Model=BBResNet18()\n",
        "          feature_extract=Model.feature_extraction(batches)\n",
        "          features.append(feature_extract)\n",
        "        print(len(features))\n",
        "        features=np.array(features)\n",
        "        np.save(('feature_set_'+str(x)+'.npy'),features)\n",
        "        \n",
        "    \n",
        "        return features   \n",
        "\n",
        "\n",
        "def main():\n",
        "  \n",
        "  file_1=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_original_1.npy')\n",
        "  file_2=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_augmented_1.npy')\n",
        "  file_3=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_original_2.npy')\n",
        "  file_4=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_augmented_2.npy')\n",
        "  file_5=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_original_3.npy')\n",
        "  file_6=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_augmented_3.npy')\n",
        "  file_7=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_original_4.npy')\n",
        "  file_8=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_augmented_4.npy')\n",
        "  file_9=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_original_5.npy')\n",
        "  file_10=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_augmented_5.npy')\n",
        "  dataset_original=np.concatenate((file_1,file_3,file_5,file_7,file_9), axis=1)\n",
        "  dataset_augmented=np.concatenate((file_2,file_4,file_6,file_8,file_10), axis=1)\n",
        "  labels_1=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/Labels/labels_1.npy')\n",
        "  labels_2=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/Labels/labels_2.npy')\n",
        "  labels_3=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/Labels/labels_3.npy')\n",
        "  labels_4=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/Labels/labels_4.npy')\n",
        "  labels_5=np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/Labels/labels_5.npy')\n",
        "  label_1=labels_1.tolist()\n",
        "  label_2=labels_2.tolist()\n",
        "  label_3=labels_3.tolist()\n",
        "  label_4=labels_4.tolist()\n",
        "  label_5=labels_5.tolist()\n",
        "  labels=label_1+label_1+label_2+label_2+label_3+label_3+label_4+label_4+label_5+label_5\n",
        "  # print('Shape of dataset_original',dataset_original.shape)\n",
        "  # print('Shape of dataset_augmented',dataset_augmented.shape)\n",
        "  #print('Shape of labels',labels.shape)\n",
        "  print('Type of labels',type(label_1))\n",
        "  print('label 1:',labels[:5])\n",
        "\n",
        "  test_data='/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-py/test_batch'\n",
        "  image_data_1=read_data(test_data)\n",
        "  test_data_1 = np.load('/content/drive/MyDrive/Colab Notebooks/Augmented Dataset/ImageSet_test_5.npy')\n",
        "  test_labels_1=np.array(image_data_1[b'labels'])\n",
        "\n",
        "## Below code is for feature extraction and saving on a file. To be run once\n",
        "# ---------------------------------------------------------------------------\n",
        "  # feature_set=[]\n",
        "  # test_set=[]\n",
        "  # test_set.append(BBResNet18.feature_resize(test_data_1,10))\n",
        "  # test_set=np.array(test_set)\n",
        "  #np.save('feature_test_set.npy', test_set)\n",
        "  # for j,i in zip(range(5),range(0,10,2)):\n",
        "  #   feature_set.append(BBResNet18.feature_resize((dataset_original[j]),i))\n",
        "  #   feature_set.append(BBResNet18.feature_resize((dataset_augmented[j]),i+1))\n",
        "  # feature_set=np.array(feature_set)\n",
        "  # np.save('Feature_set.npy',feature_set)\n",
        "  # print('Feature vector size:',len(feature_set))\n",
        "  # print('Feature vector shape:',feature_set.shape)\n",
        "    \n",
        "  classes='/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-py/batches.meta'\n",
        "  image_data_1=read_data(classes)\n",
        "  classes_1=image_data_1[b'label_names']\n",
        "  name=[]\n",
        "  for i in range(10):\n",
        "    classes=classes_1[i].decode()\n",
        "    name.append(classes)\n",
        "  num_classes=np.array(name)\n",
        "  print('Type of Num Class:',type(num_classes))\n",
        "  print('First of Num Class:',(num_classes[0]))\n",
        "\n",
        "  ############\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Activation\n",
        "  from keras.optimizers import gradient_descent_v2\n",
        "\n",
        "  class ReLU():\n",
        "    def __init__(self):\n",
        "      pass\n",
        "\n",
        "    def forward(self, input):\n",
        "      relu_forward = np.maximum(0,input)\n",
        "      return relu_forward\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "      relu_grad = input > 0\n",
        "      return grad_output*relu_grad\n",
        "\n",
        "  def binary_crossentropy(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss.mean()\n",
        "\n",
        "  def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "  def feedforward(x, weights1, biases1, weights2, biases2, weights3, biases3):\n",
        "    z1 = np.dot(x, weights1) + biases1\n",
        "    a1 = ReLU.forward(z1)\n",
        "    z2 = np.dot(a1, weights2) + biases2\n",
        "    a2 = ReLU.forward(z2)\n",
        "    z3 = np.dot(a2, weights3) + biases3\n",
        "    output = softmax(z3)\n",
        "    return output\n",
        "\n",
        "  def gradients(weights, biases, X_train, y_train):\n",
        "    # Calculate the dot product of the inputs and weights\n",
        "    weighted_sum = np.dot(X_train, weights) + biases\n",
        "    \n",
        "    # Pass the dot product through the activation function (sigmoid)\n",
        "    y_pred = softmax(weighted_sum)\n",
        "    \n",
        "    # Calculate the error (binary cross-entropy loss)\n",
        "    loss = binary_crossentropy(y_train, y_pred)\n",
        "    \n",
        "    # Calculate the gradient of the error with respect to the weights\n",
        "    d_weights = np.dot(X_train.T, (y_pred - y_train))\n",
        "    \n",
        "    # Calculate the gradient of the error with respect to the biases\n",
        "    d_biases = np.sum(y_pred - y_train, axis=0)\n",
        "    \n",
        "    return d_weights, d_biases, loss\n",
        "    d_weights, d_biases, loss = gradients(weights, biases, X, y_true)\n",
        "    print(\"Gradient of weights:\", d_weights)\n",
        "    print(\"Gradient of biases:\", d_biases)\n",
        "    print(\"Loss:\", loss)\n",
        "\n",
        "  X_test=np.reshape((np.load('/content/drive/MyDrive/Colab Notebooks/Test_data/feature_set_10.npy')),(10000,512))\n",
        "  X_test=X_test.astype('float32')\n",
        "  Y_test=test_labels_1\n",
        "  X_test /=255\n",
        "  Y_test=keras.utils.to_categorical(Y_test,10)\n",
        "  for i in range(10):\n",
        "    feature_set=np.load('/content/drive/MyDrive/Colab Notebooks/Features/feature_set_'+str(i)+'.npy')\n",
        "    #print('Shape of feature_set:',feature_set.shape) \n",
        "    X_train=np.reshape((feature_set),(10000,512))\n",
        "    labels=np.array(labels)\n",
        "    y_train=np.reshape(labels, (10,10000))\n",
        "    #print('First element of Ytrain:',y_train[0])\n",
        "    Y_train=y_train[0]\n",
        "    X_train=X_train.astype('float32')\n",
        "  \n",
        "  # print(X_train.shape)\n",
        "  # print(Y_train.shape)\n",
        "  # print(X_test.shape)\n",
        "  # print(Y_test.shape)\n",
        "  # print(X_test.dtype)\n",
        "  # print(X_train.dtype)\n",
        "    X_train /=255\n",
        "  \n",
        "########### Below code is to see the alignment of images with their respective labels\n",
        "\n",
        "  # print('Example training images and their labels: ' + str([x for x in Y_test[0:5]])) \n",
        "  # print('Corresponding classes for the labels: ' + str([num_classes[x] for x in Y_test[0:5]]))\n",
        "\n",
        "  # f, axarr = plt.subplots(1, 5)\n",
        "  # f.set_size_inches(32, 16)\n",
        "\n",
        "  # for i in range(5):\n",
        "  #   img = test_data_1[i]\n",
        "  #   axarr[i].imshow(img)\n",
        "  # plt.show()\n",
        "############\n",
        "    Y_train=keras.utils.to_categorical(Y_train,10)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=512))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    # Update the weights and biases\n",
        "    weights_1 = np.random.rand(512,64)\n",
        "    biases_1 = np.random.rand(64)\n",
        "    weights_2 = np.random.rand(64,64)\n",
        "    biases_2 = np.random.rand(64)\n",
        "    weights_3 = np.random.rand(64,10)\n",
        "    biases_3 = np.random.rand(10)\n",
        "    print('New Weight:',weights_1)\n",
        "    print('New biases:',biases_1)\n",
        "    # To set the weights and biases of a particular layer\n",
        "    # model.layers[0].set_weights([weights_1, biases_1])\n",
        "    # model.layers[1].set_weights([weights_2, biases_2])\n",
        "    # model.layers[2].set_weights([weights_3, biases_3])\n",
        "\n",
        "    # Use the predict method to get the predicted output\n",
        "    y_pred = model.predict(X_train)\n",
        "\n",
        "    # Round the predicted output to get the binary classification\n",
        "    y_pred = np.round(y_pred)\n",
        "\n",
        "    # Calculate the error by subtracting the predicted output from the actual output\n",
        "    error = y_pred - Y_train\n",
        "    print('Error for this epoch:',error.shape)\n",
        "    \n",
        "#####\n",
        "    \n",
        "    sgd = gradient_descent_v2.SGD(learning_rate=0.002, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "    model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  \n",
        "    history = model.fit(X_train,Y_train, epochs=20, batch_size=512, verbose=1, validation_split=0.2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "  weights, biases = model.layers[0].get_weights()\n",
        "  np.save('final_weights.npy', weights)\n",
        "  print(\"Initial weights:\", weights)\n",
        "  print(\"Initial biases:\", biases)\n",
        "\n",
        "  score = model.evaluate(X_test, Y_test, batch_size=512, verbose=0)\n",
        "  print(model.metrics_names)\n",
        "  print('MLP model output:',score)\n",
        "\n",
        "\n",
        "############ below code is for implementation of SVM classifier\n",
        "  from sklearn import svm\n",
        "# Train the SVM classifier\n",
        "  for i in range(10):\n",
        "    feature_set=np.load('/content/drive/MyDrive/Colab Notebooks/Features/feature_set_'+str(i)+'.npy')\n",
        "    X_train=np.reshape((feature_set),(10000,512))\n",
        "    y_train=np.reshape((Y_train),(10000,1))\n",
        "    clf = svm.SVC(kernel='rbf', C=1, gamma='scale')\n",
        "    clf.fit(X_train, y_train)\n",
        "# Validate the classifier\n",
        "  accuracy = clf.score(X_test, Y_test)\n",
        "  print(\"SVM Accuracy:\", accuracy)\n",
        "# Make predictions\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "###### below code is for implementation of KNN classifier\n",
        "\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  for i in range(10):\n",
        "    feature_set=np.load('/content/drive/MyDrive/Colab Notebooks/Features/feature_set_'+str(i)+'.npy')\n",
        "    X_train=np.reshape((feature_set),(10000,512))\n",
        "    \n",
        "    knn5 = KNeighborsClassifier(n_neighbors = 5)\n",
        "    knn1 = KNeighborsClassifier(n_neighbors=1)\n",
        "    knn5.fit(X_train, Y_train)\n",
        "    knn1.fit(X_train, Y_train)\n",
        "\n",
        "  y_pred_5 = knn5.predict(X_test)\n",
        "  y_pred_1 = knn1.predict(X_test)\n",
        "\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  print(\"KNN Accuracy with k=5\", accuracy_score(Y_test, y_pred_5)*100)\n",
        "  print(\"KNN Accuracy with k=1\", accuracy_score(Y_test, y_pred_1)*100)\n",
        "\n",
        "####### below code is for implementation of Logistic Regression classifier\n",
        "\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  from sklearn import preprocessing\n",
        "  #from sklearn.inspection import DecisionBoundaryDisplay\n",
        "  for i in range(10):\n",
        "    feature_set=np.load('/content/drive/MyDrive/Colab Notebooks/Features/feature_set_'+str(i)+'.npy')\n",
        "    X_train=np.reshape((feature_set),(10000,512))\n",
        "    scaler = preprocessing.StandardScaler().fit(X_train) \n",
        "    X_scaled = scaler.transform(X_train)\n",
        "    logreg = LogisticRegression(C=1e5)\n",
        "    logreg.fit(X_scaled, Y_train)\n",
        "  predictions = logreg.predict(X_test)\n",
        "  score = logreg.score(X_test, Y_test)\n",
        "  print('Logistic Regression Accuracy:',score)\n",
        "\n",
        "\n",
        "##### below code is for implementation of Decision Tree classifier\n",
        "  import pandas as pd\n",
        "  from sklearn.tree import DecisionTreeClassifier\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn import metrics\n",
        "\n",
        "  for i in range(10):\n",
        "    feature_set=np.load('/content/drive/MyDrive/Colab Notebooks/Features/feature_set_'+str(i)+'.npy')\n",
        "    X_train=np.reshape((feature_set),(10000,512))\n",
        "# Create Decision Tree classifer object\n",
        "    clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "    clf = clf.fit(X_train,Y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  # Model Accuracy, how often is the classifier correct?\n",
        "  print(\"Decision Tree Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "  # function call\n",
        "  main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "K9E-fY9rYhqH",
        "outputId": "60ea010b-08b5-43b9-f074-61ae0f8cabbc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of labels <class 'list'>\n",
            "label 1: [6, 9, 9, 4, 1]\n",
            "Type of Num Class: <class 'numpy.ndarray'>\n",
            "First of Num Class: airplane\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-758f8c6aba5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m   \u001b[0;31m# function call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-758f8c6aba5d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mfeature_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Features/feature_set_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_train' is not defined"
          ]
        }
      ]
    }
  ]
}